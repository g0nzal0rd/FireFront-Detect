{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Global System.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6da87a322dad4448bd7e553301d63b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74ad05e08262483180c517164bb7c199",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da0cf2ebbad34fe19c007ae8ab7a010f",
              "IPY_MODEL_5996c374456245f999197f1a0c69e96c"
            ]
          }
        },
        "74ad05e08262483180c517164bb7c199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da0cf2ebbad34fe19c007ae8ab7a010f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d850d19eb9d4c1d8af40ccbc24d0c1d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_154b63a10a174e24a4b3dd8298029bb0"
          }
        },
        "5996c374456245f999197f1a0c69e96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7631730ec6f34bada254568a1ff76476",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:02&lt;00:00,  1.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d95f82d5ef774f28b3e442469e68bed5"
          }
        },
        "4d850d19eb9d4c1d8af40ccbc24d0c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "154b63a10a174e24a4b3dd8298029bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7631730ec6f34bada254568a1ff76476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d95f82d5ef774f28b3e442469e68bed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1F6eIRu0fmR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "3525119b-31b7-40d7-9e01-7c5ff113ed19"
      },
      "source": [
        " import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from skimage.util.shape import view_as_windows\n",
        "from keras.preprocessing import image\n",
        "from skimage.util import crop\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "!pip install image_slicer\n",
        "import image_slicer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/squeezenet_final.h5\" \"/content/squeezenet.h5\"\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/u-net_patch2.h5\" \"/content/u-net.h5\"\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/smoke_test.zip\" \"/content/test_set.zip\"\n",
        "#!unzip test_set.zip\n",
        "%cd /content\n",
        "\n",
        "seg_model = load_model(\"u-net.h5\")\n",
        "class_model = load_model(\"squeezenet.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting image_slicer\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/aa/fcda7889ea0b248310b1cc67b98a7f398b38e0ea672eb95b17cbfd5d06b6/image_slicer-2.1.1-py2.py3-none-any.whl\n",
            "Collecting Pillow==7.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 14.9MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow, image-slicer\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-7.2.0 image-slicer-2.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhsvnrX-v2nE"
      },
      "source": [
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/squeezenet_final.h5\" \"/content/squeezenet.h5\"\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/u-net_patch2.h5\" \"/content/u-net.h5\"\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/smoke_test.zip\" \"/content/test_set.zip\"\n",
        "#!unzip test_set.zip\n",
        "%cd /content\n",
        "\n",
        "seg_model = load_model(\"u-net.h5\")\n",
        "class_model = load_model(\"squeezenet.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTnNdQLt69Eq"
      },
      "source": [
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/u-net_smoke5.h5\" \"/content/u-net.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBYp9xTUIV5o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "026295ed-51fa-400a-de6b-62585652dc87"
      },
      "source": [
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/fire_test.zip\" \"/content/test_set.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/fire_test.zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewIW7Gt8B5CK"
      },
      "source": [
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/fire test.zip\" \"/content/fire_test.zip\"\n",
        "%cp \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/smoke test.zip\" \"/content/smoke_test.zip\"\n",
        "!unzip fire_test.zip\n",
        "!unzip smoke_test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFtGRlgbCGZK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYgRMYD21lb"
      },
      "source": [
        "def prepare_image (filename, size):\n",
        "  img = image.load_img(filename)\n",
        "  img = img.resize(size)\n",
        "  img = np.asarray(img)\n",
        "  x_img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  return img,x_img\n",
        "\n",
        "def classification_predicition(image):\n",
        "  fire = 0\n",
        "  output = class_model.predict(image)\n",
        "  if output[0][0] > class_threshold:\n",
        "    fire = 1\n",
        "  else:\n",
        "    fire = 0\n",
        "  return fire\n",
        "\n",
        "input_size = (128,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g368PsTj0ZSs"
      },
      "source": [
        "# Function for Predicition using patch sliding\n",
        "\n",
        "def image_slicer_prediction (filename,n):\n",
        "  square_h = 128*n\n",
        "  square_w = 128*n\n",
        "\n",
        "  img = image.load_img(filename)\n",
        "\n",
        "  img = np.asarray(img)\n",
        "  x_img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  size = img.shape\n",
        "  #print (\"original size:\") \n",
        "  #print(size)\n",
        "  squares_in_y = size[0]/square_h\n",
        "  squares_in_x = size[1]/square_w\n",
        "  #print(squares_in_x)\n",
        "  #print(squares_in_y)\n",
        "\n",
        "  squares_in_x = round(squares_in_x)\n",
        "  squares_in_y = round(squares_in_x)\n",
        "  target_size = (squares_in_y*square_h,squares_in_x*square_w)\n",
        "  #print(target_size)\n",
        "\n",
        "  img = image.load_img(filename,target_size)\n",
        "  img = np.asarray(img)\n",
        "  img_compressed = image.load_img(filename,(squares_in_x*128,squares_in_y*128))\n",
        "\n",
        "  if squares_in_x > 1 and squares_in_y > 1:\n",
        "    tiles = image_slicer.slice(filename,squares_in_y*squares_in_x, save=False)\n",
        "    slide = 1\n",
        "    #print (\"sliding\")\n",
        "  else:\n",
        "    slide = 0\n",
        "    #print (\"no sliding\")\n",
        "\n",
        "  i = 0\n",
        "  if slide == 0:\n",
        "\n",
        "    img, x_img = prepare_image(filename,input_size)\n",
        "    fire = classification_predicition(x_img)\n",
        "    \n",
        "    if ( fire == 1):\n",
        "      #print(\"fire in patch:\" ,i)\n",
        "      prediction = seg_model.predict(x_img/255)\n",
        "      new_image = np.squeeze((prediction), axis=0)\n",
        "\n",
        "      pred_bin = (prediction > seg_threshold).astype(np.uint8)\n",
        "      pred_bin = np.squeeze((pred_bin), axis=0)\n",
        "\n",
        "      prediction_image = array_to_img(new_image)\n",
        "      prediction_image = prediction_image.resize([size[1],size[0]])\n",
        "      bin_image = array_to_img(pred_bin)\n",
        "      bin_image = bin_image.resize([size[1],size[0]])\n",
        "      return (img_compressed,prediction_image,bin_image)\n",
        "\n",
        "    else:\n",
        "      new_image_x = Image.new('L', (128,128))\n",
        "      new_image_x = new_image_x.resize([size[1],size[0]])\n",
        "      return (img_compressed,new_image_x,new_image_x)\n",
        "      \n",
        "\n",
        "  if slide == 1:\n",
        "    new_image_pred = Image.new('L', (squares_in_x*128,squares_in_y*128))\n",
        "    new_image_pred_bin = Image.new('L', (squares_in_x*128,squares_in_y*128))\n",
        "    for tile in tiles:\n",
        "        img = tile.image\n",
        "        img = img.resize((128,128))\n",
        "        img = np.asarray(img)\n",
        "        x_img = np.expand_dims(img, axis=0)\n",
        "      \n",
        "        fire = classification_predicition(x_img)\n",
        "        \n",
        "        if ( fire == 1):\n",
        "          #print(\"fire in patch: \" ,i)\n",
        "          prediction = seg_model.predict(x_img/255)\n",
        "          new_image = np.squeeze((prediction), axis=0)\n",
        "\n",
        "          pred_bin = (prediction > seg_threshold).astype(np.uint8)\n",
        "          pred_bin = np.squeeze((pred_bin), axis=0)\n",
        "      \n",
        "          new_image_pred.paste(array_to_img(new_image), ( (i*128 )%(squares_in_x*128), ( 128*int((i*128)/(squares_in_x*128)))) )\n",
        "          new_image_pred_bin.paste(array_to_img(pred_bin), ( (i*128 )%(squares_in_x*128), ( 128*int((i*128)/(squares_in_x*128)))) )\n",
        "        i += 1\n",
        "    \n",
        "  new_image_pred = new_image_pred.resize([size[1],size[0]])\n",
        "  new_image_pred_bin = new_image_pred_bin.resize([size[1],size[0]])\n",
        "\n",
        "  return (img_compressed,new_image_pred,new_image_pred_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBNLqfLPVSbU"
      },
      "source": [
        "# Predicition using Quad_Tree Method\n",
        "import numpy as np\n",
        "from skimage.util.shape import view_as_windows\n",
        "from keras.preprocessing import image\n",
        "from skimage.util import crop\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "import image_slicer\n",
        "\n",
        "def image_quad_prediction (filename, ratio, min_patch_size):\n",
        "\n",
        "  orig_image = image.load_img(filename)\n",
        "  \n",
        "  square_image = orig_image.resize((128,128))\n",
        "  square_image_class = orig_image.resize((192,192))\n",
        "\n",
        "  orig_image = np.asarray(orig_image)\n",
        "  square_image = np.asarray(square_image)\n",
        "\n",
        "  square_image_class = np.asarray(square_image_class)\n",
        "  \n",
        "  x_orig_image = np.expand_dims(square_image, axis=0)\n",
        "  x_square_image_class = np.expand_dims(square_image, axis=0)\n",
        "  size = orig_image.shape\n",
        "  #print(size)\n",
        "\n",
        "  if ( class_threshold > 0):\n",
        "    fire = class_model.predict(x_square_image_class)\n",
        "\n",
        "  prediction = seg_model.predict(x_orig_image/255)\n",
        "\n",
        "  pred_bin = (prediction > seg_threshold).astype(np.uint8)\n",
        "\n",
        "  elements = size[0]*size[1]\n",
        "\n",
        "  if ( (prediction>seg_threshold).sum() < ratio * elements) and ( size[0] > min_patch_size and size[1] > min_patch_size ):\n",
        "    #print((prediction>threshold).sum() )\n",
        "    #print (\"sliding\")\n",
        "    new_image_pred = Image.new('L', (size[1],size[0]))\n",
        "    new_image_bin = Image.new('L', (size[1],size[0]))\n",
        "\n",
        "    tiles = image_slicer.slice(filename, 4 , save=False)\n",
        "    i = 0\n",
        "    for tile in tiles:\n",
        "      tile.image.save(\"temp.png\")\n",
        "      result, pred_bin = image_quad_prediction (\"temp.png\", ratio, min_patch_size)\n",
        "      if i == 0:\n",
        "        new_image_pred.paste(result, ( 0 , 0))\n",
        "        new_image_bin.paste(pred_bin, ( 0 , 0))\n",
        "      elif i == 1:\n",
        "        new_image_pred.paste(result, ( round(size[1]/2)-1,  0  ) )\n",
        "        new_image_bin.paste(pred_bin, ( round(size[1]/2)-1,  0  ) )\n",
        "      elif i == 2:\n",
        "        new_image_pred.paste(result, ( 0 , round(size[0]/2)-1))\n",
        "        new_image_bin.paste(pred_bin, ( 0 , round(size[0]/2)-1))\n",
        "      else :\n",
        "        new_image_pred.paste(result, ( round( size[1]/2)-1, round(size[0]/2)-1 )) #\n",
        "        new_image_bin.paste(pred_bin, ( round( size[1]/2)-1, round(size[0]/2)-1 )) #\n",
        "\n",
        "      i = i+1\n",
        "\n",
        "  else:\n",
        "    if ( class_threshold == 0 or fire[0][0] > class_threshold  ) :\n",
        "      new_image = np.squeeze((prediction), axis=0)\n",
        "      new_image = array_to_img(new_image)\n",
        "      new_image = new_image.resize([size[1],size[0]])\n",
        "\n",
        "      pred_bin = np.squeeze((pred_bin), axis=0)\n",
        "      pred_bin = array_to_img(pred_bin)\n",
        "      pred_bin = pred_bin.resize([size[1],size[0]])\n",
        "\n",
        "    else:\n",
        "      new_image = Image.new('L', (size[1],size[0]))\n",
        "      pred_bin = Image.new('L', (size[1],size[0]))\n",
        "    return new_image, pred_bin\n",
        "\n",
        "  \n",
        "  return new_image_pred, new_image_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eet8iSoma-tB"
      },
      "source": [
        "# Calculate Performance metrics between images \n",
        "def Calculate_Performance(i):\n",
        "  masks_path = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/fire test/masks\"\n",
        "  result_folder = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/fire test/results\"\n",
        "  ids = next(os.walk(masks_path)) \n",
        "  ids = ids[2]\n",
        "  #print(\"No. of images = \", len(ids))\n",
        "\n",
        "  pos_iou_score_total = 0\n",
        "  neg_iou_score_total  = 0\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "\n",
        "  total=len(ids)\n",
        "\n",
        "  pos_iou_score = np.zeros(total)\n",
        "  neg_iou_score = np.zeros(total)\n",
        "  avg_iou_sing = np.zeros(total)\n",
        "\n",
        "  for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "      # Load images\n",
        "      mask_file = masks_path+\"/\"+id_\n",
        "      result_file = result_folder+\"/\"+id_\n",
        "\n",
        "      mask = image.load_img(mask_file)\n",
        "      mask = np.asarray(mask)\n",
        "\n",
        "      result = image.load_img(result_file)\n",
        "      result = np.asarray(result) \n",
        "\n",
        "      pos_intersection = np.logical_and(mask, result)\n",
        "      pos_union = np.logical_or(mask, result)\n",
        "      pos_iou_score[n] = np.sum(pos_intersection) / np.sum(pos_union)\n",
        "\n",
        "      neg_intersection = np.logical_and(1 - mask, 1- result)\n",
        "      neg_union = np.logical_or(1-mask, 1- result)\n",
        "      neg_iou_score[n] = np.sum(neg_intersection) / np.sum(neg_union)\n",
        "\n",
        "      if np.isnan(pos_iou_score[n]): (pos_iou_score[n]) = neg_iou_score[n]\n",
        "\n",
        "      avg_iou_sing[n] = ( pos_iou_score[n] + neg_iou_score[n] ) / 2\n",
        "      #print(avg_iou_sing[n])\n",
        "\n",
        "      TP = TP + np.sum(pos_intersection)\n",
        "      TN = TN + np.sum(neg_intersection)\n",
        "      FP = FP + np.sum(np.logical_and(1-mask, result))\n",
        "      FN = FN + np.sum(np.logical_and(mask, 1-result))\n",
        "\n",
        "      \n",
        "  pos_avg_iou = np.mean(pos_iou_score)\n",
        "  neg_avg_iou = np.mean(neg_iou_score)\n",
        "  avg_iou = (pos_avg_iou+neg_avg_iou)/2\n",
        "\n",
        "  print(avg_iou_sing)\n",
        "\n",
        "\n",
        "  stand_deviation = np.std(avg_iou_sing)\n",
        "\n",
        "\n",
        "  acc = (TP + TN)/(TP+TN+FP+FN)\n",
        "  prec = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "\n",
        "\n",
        "  #print(\"positive avg iou:\", pos_avg_iou)\n",
        "  #print(\"negative avg iou:\", neg_avg_iou)\n",
        "  #print(\"avg iou:\", avg_iou)\n",
        "  #print(\"TP:\", TP, \"TN:\", TN, \"FP\", FP, \"FN\", FN)\n",
        "  #print(\"Acc:\", acc, \"Prec:\", prec, \"Recall:\", recall)\n",
        "  #print(\"std. dev. of avg iou:\", stand_deviation)\n",
        "\n",
        "  \n",
        "  return avg_iou, avg_iou_sing, acc, stand_deviation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms4jH5SvkrGf"
      },
      "source": [
        "# Predict Images in a Folder\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "folder_path = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/Firefront Dataset\"\n",
        "result_folder = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/Firefront Dataset Results\"\n",
        "ids = next(os.walk(folder_path)) \n",
        "ids = ids[2]\n",
        "print(\"No. of images = \", len(ids))\n",
        "\n",
        "best_avg_iou = 0;\n",
        "best_seg_threshold = 0;\n",
        "best_class_threshold = 0;\n",
        "best_ratio = 0;\n",
        "best_size = 0;\n",
        "\n",
        "min_patch_size = 400;\n",
        "\n",
        "for class_threshold_ori in [0.4]:    #0.3\n",
        "  for seg_threshold in [0.4]:            #0.4\n",
        "    for ratio in [ 0.01 ]:  #0.00001 \n",
        "\n",
        "\n",
        "\n",
        "        print (\"class_threshold:\",class_threshold_ori )\n",
        "        print (\"seg_threshold:\",seg_threshold )\n",
        "        print (\"ratio:\",ratio )\n",
        "        \n",
        "        print(\"\\n############# Q+S ################\\n\")\n",
        "\n",
        "        class_threshold = 0.0\n",
        "\n",
        "        for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "          # Load images\n",
        "          filename = folder_path+\"/\"+id_\n",
        "          global_image, pred_bin = image_quad_prediction(filename, ratio, min_patch_size)\n",
        "          #img_compressed, global_image, pred_bin = image_slicer_prediction (filename,100000)\n",
        "          pred_bin.save(result_folder+\"/\"+id_)\n",
        "                  \n",
        "        qs_avg_iou, qs_avg_iou_sing, qs_acc, qs_stand_deviation = Calculate_Performance(0)\n",
        "\n",
        "        print(\"\\n############# C+S ################\\n\")\n",
        "\n",
        "        class_threshold = class_threshold_ori\n",
        "\n",
        "        for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "            # Load images\n",
        "            filename = folder_path+\"/\"+id_\n",
        "            #global_image, pred_bin = image_quad_prediction(filename)\n",
        "            img_compressed, global_image, pred_bin = image_slicer_prediction (filename,100000)\n",
        "            pred_bin.save(result_folder+\"/\"+id_)\n",
        "                  \n",
        "        cs_avg_iou, cs_avg_iou_sing, cs_acc, cs_stand_deviation = Calculate_Performance(0)\n",
        "\n",
        "        print(\"\\n############# S ################\\n\")\n",
        "        class_threshold = 0.0\n",
        "\n",
        "        for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "            # Load images\n",
        "            filename = folder_path+\"/\"+id_\n",
        "            #global_image, pred_bin = image_quad_prediction(filename)\n",
        "            img_compressed, global_image, pred_bin = image_slicer_prediction (filename,100000)\n",
        "            pred_bin.save(result_folder+\"/\"+id_)\n",
        "                  \n",
        "        s_avg_iou, s_avg_iou_sing, s_acc, s_stand_deviation = Calculate_Performance(0)\n",
        "\n",
        "        print(\"\\n############# Q+C+S ################\\n\")\n",
        "\n",
        "        class_threshold = class_threshold_ori\n",
        "\n",
        "        for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "            # Load images\n",
        "            filename = folder_path+\"/\"+id_\n",
        "            global_image, pred_bin = image_quad_prediction(filename, ratio, min_patch_size)\n",
        "            #img_compressed, global_image, pred_bin = image_slicer_prediction (filename,100000)\n",
        "            pred_bin.save(result_folder+\"/\"+id_)\n",
        "                  \n",
        "        qcs_avg_iou, qcs_avg_iou_sing, qcs_acc, qcs_stand_deviation = Calculate_Performance(0)\n",
        "\n",
        "        data = (qcs_avg_iou_sing, qs_avg_iou_sing, cs_avg_iou_sing, s_avg_iou_sing)\n",
        "\n",
        "        if qcs_avg_iou > best_avg_iou:\n",
        "            best_avg_iou = qcs_avg_iou\n",
        "            best_seg_threshold = seg_threshold\n",
        "            best_class_threshold = class_threshold\n",
        "            best_ratio = ratio\n",
        "            best_size = min_patch_size\n",
        "\n",
        "        print(\"\\n#############################################\\nQ+C+S:\\n Avg.Iou: \",qcs_avg_iou, \" SD: \", qcs_stand_deviation,\" Acc: \", qcs_acc )\n",
        "        print(\"Q+S:\\n Avg.Iou: \",qs_avg_iou, \" SD: \", qs_stand_deviation,\" Acc: \", qs_acc )\n",
        "        print(\"C+S:\\n Avg.Iou: \",cs_avg_iou, \" SD: \", cs_stand_deviation,\" Acc: \", cs_acc )\n",
        "        print(\"S:\\n Avg.Iou: \",s_avg_iou, \" SD: \", s_stand_deviation,\" Acc: \", s_acc )\n",
        "\n",
        "      \n",
        "print(\"best avg_iou:\", best_avg_iou )\n",
        "print(\"best_seg_threshold:\", best_seg_threshold )\n",
        "print(\"best_class_threshold:\", best_class_threshold )\n",
        "print(\"best_ratio\", best_ratio)\n",
        "\n",
        "# Creating plot \n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax = fig.add_axes([0, 0, 1, 1]) \n",
        "bp = ax.boxplot(data )\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Avg. Iou')\n",
        "plt.title('Box Plot of Avg. IoU Distribution')\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjDy4Ki8Oofi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "6da87a322dad4448bd7e553301d63b6a",
            "74ad05e08262483180c517164bb7c199",
            "da0cf2ebbad34fe19c007ae8ab7a010f",
            "5996c374456245f999197f1a0c69e96c",
            "4d850d19eb9d4c1d8af40ccbc24d0c1d",
            "154b63a10a174e24a4b3dd8298029bb0",
            "7631730ec6f34bada254568a1ff76476",
            "d95f82d5ef774f28b3e442469e68bed5"
          ]
        },
        "outputId": "89cc8c57-51de-43cb-a73c-fecc53e0369d"
      },
      "source": [
        "# Predict Images in a Folder\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "folder_path = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/Firefront Dataset\"\n",
        "result_folder = \"/content/drive/My Drive/GoncÌ§alo/IST/FIREFRONT/CNN/Firefront Dataset Smoke Results\"\n",
        "ids = next(os.walk(folder_path)) \n",
        "ids = ids[2]\n",
        "print(\"No. of images = \", len(ids))\n",
        "\n",
        "best_avg_iou = 0;\n",
        "best_seg_threshold = 0;\n",
        "best_class_threshold = 0;\n",
        "best_ratio = 0;\n",
        "best_size = 0;\n",
        "\n",
        "min_patch_size = 700;\n",
        "\n",
        "for class_threshold_ori in [0.4]:    #0.3\n",
        "  for seg_threshold in [0.4]:            #0.4\n",
        "    for ratio in [ 0.001 ]:  #0.00001 \n",
        "\n",
        "#for class_threshold_ori in [0.3,0.4, 0.5, 0.6]:    #0.3\n",
        "  #for seg_threshold in [0.3,0.4, 0.5, 0.6]:            #0.4\n",
        "    #for ratio in [ 0.00001 , 0.0001 , 0.001]:  #0.00001\n",
        "      #for min_patch_size in [ 200, 300, 400 ]:\n",
        "\n",
        "        print (\"class_threshold:\",class_threshold_ori )\n",
        "        print (\"seg_threshold:\",seg_threshold )\n",
        "        print (\"ratio:\",ratio )\n",
        "\n",
        "       \n",
        "        class_threshold = class_threshold_ori\n",
        "\n",
        "        for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "            # Load images\n",
        "            filename = folder_path+\"/\"+id_\n",
        "            global_image, pred_bin = image_quad_prediction(filename, ratio, min_patch_size)\n",
        "            #img_compressed, global_image, pred_bin = image_slicer_prediction (filename,100000)\n",
        "            pred_bin.save(result_folder+\"/\"+id_)\n",
        "                  \n",
        "\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "No. of images =  4\n",
            "class_threshold: 0.0\n",
            "seg_threshold: 0.8\n",
            "ratio: 1e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6da87a322dad4448bd7e553301d63b6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-41GEOhpEAr",
        "outputId": "71d8b1a0-834f-46ac-a7b9-eba13d716ecc"
      },
      "source": [
        "#Predict A single Image\n",
        "filename = \"fogo_grande.jpg\"\n",
        "class_threshold = 0.4\n",
        "seg_threshold = 0.4\n",
        "global_image, pred_bin = image_quad_prediction(filename, 0.01, 4000)\n",
        "pred_bin.save(\"result.png\")\n",
        "size = pred_bin.size\n",
        "total_pixels = size[0]*size[1]\n",
        "pred_bin = np.asarray(pred_bin)\n",
        "ratio = (pred_bin/256).sum() / total_pixels\n",
        "print(\"ratio= \",ratio*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ratio=  26.021320897547838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8kkbYnNYC-m",
        "outputId": "5f59e676-fee2-4807-9ca9-c1b0c21fea2e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}